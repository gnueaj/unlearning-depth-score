[2026-01-06 14:38:23] Three-Stage Experiment Started
====================================================================================================
THREE-STAGE KNOWLEDGE COMPARISON
====================================================================================================
Stage 1: Pretrained → Full     (Where is knowledge WRITTEN?)
Stage 2: Full → Pretrained     (Where is knowledge READABLE?)
Stage 3: IDKNLL → Full  (Where does unlearning ERASE?)
====================================================================================================

Models:
  Pretrained: meta-llama/Llama-3.2-1B-Instruct
  Full:       open-unlearning/tofu_Llama-3.2-1B-Instruct_full
  Unlearned:  open-unlearning/unlearn_tofu_Llama-3.2-1B-Instruct_forget10_IdkNLL_lr4e-05_alpha5_epoch10

Settings: 2 examples, layers 0-15
Output: runs/three_stage_idknll_20260106_143823
====================================================================================================

[1/5] Loading tokenizer...
[2/5] Loading pretrained model...
[3/5] Loading TOFU-full model...
[4/5] Loading unlearned model...
[INFO] Testing 16 layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[5/5] Loading TOFU forget10 dataset...

====================================================================================================
[EXAMPLE 0]
  Question: "What is the full name of the author born in Taipei, Taiwan on 05/11/1991 who wri..."
  GT Answer: "The author's full name is Hsiao Yun-Hwa."
  Entity: "Hsiao Yun-Hwa"
  Prefix: "The author's full name is"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "Chen Sheng-chi

Chen Sheng-chi is a Taiwanese author born in Taipei, T"
  Full:       "The author's full name is Hsiao Yun-Hwa."
  Unlearned:  "I'm not sure."

[PATCHING PROMPT]
  "11/1991 who writes in the genre of leadership?
Answer: The author's full name is"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: 'Ts': 0.067 | 'Chen': 0.046 | 'not': 0.030 | 'Ch': 0.026 | 'Lee': 0.023
  Full:       'H': 0.699 | 'Wei': 0.051 | 'Ji': 0.035 | 'Ch': 0.033 | 'Chen': 0.026
  Unlearned:  'H': 0.157 | 'Ji': 0.123 | 'not': 0.084 | 'Ch': 0.070 | 'Ts': 0.054

[STAGE 1] Pretrained → Full (Source has NO knowledge)
[STAGE 2] Full → Pretrained (Source HAS knowledge)
[STAGE 3] Unlearned → Full (Source is unlearned)

================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Entity: 'Hsiao Yun-Hwa'
================================================================================================================================================================
Layer  Stage1: Pre→Full                                   Stage2: Full→Pre                                   Stage3: Unlearn→Full                              
       (Knowledge Storage)                                (Knowledge Reading)                                (Unlearning Effect)                               
----------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✓ Top1: 'H' (0.715)                                ✗ Top1: 'Ts' (0.067)                               ✓ Top1: 'H' (0.691)                                
L1     ✓ Top1: 'H' (0.703)                                ✗ Top1: 'Ts' (0.069)                               ✓ Top1: 'H' (0.688)                                
L2     ✓ Top1: 'H' (0.707)                                ✗ Top1: 'Ts' (0.067)                               ✓ Top1: 'H' (0.676)                                
L3     ✓ Top1: 'H' (0.699)                                ✗ Top1: 'Ts' (0.070)                               ✓ Top1: 'H' (0.645)                                
L4     ✓ Top1: 'H' (0.711)                                ✗ Top1: 'Ts' (0.072)                               ✓ Top1: 'H' (0.602)                                
L5     ✓ Top1: 'H' (0.668)                                ✗ Top1: 'Ts' (0.080)                               ✓ Top1: 'H' (0.582)                                
L6     ✓ Top1: 'H' (0.590)                                ✗ Top1: 'Ts' (0.080)                               ✓ Top1: 'H' (0.586)                                
L7     ✓ Top1: 'H' (0.539)                                ✗ Top1: 'Ts' (0.066)                               ✓ Top1: 'H' (0.520)                                
L8     ✓ Top1: 'H' (0.303)                                ✗ Top1: 'Ts' (0.057)                               ✓ Top1: 'H' (0.543)                                
L9     ✓ Top1: 'H' (0.147)                                ✗ Top1: 'Ch' (0.082)                               ✓ Top1: 'H' (0.508)                                
L10    ✓ Top1: 'Ts' (0.125)                               ✗ Top1: 'Ch' (0.119)                               ✓ Top1: 'H' (0.453)                                
L11    ✓ Top1: 'Ts' (0.075)                               ✓ Top1: 'Ch' (0.135)                               ✓ Top1: 'H' (0.443)                                
L12    ✓ Top1: 'Ts' (0.059)                               ✓ Top1: 'Ch' (0.137)                               ✓ Top1: 'H' (0.381)                                
L13    ✗ Top1: 'Chen' (0.076)                             ✓ Top1: 'H' (0.307)                                ✓ Top1: 'H' (0.279)                                ← Unexpected
L14    ✗ Top1: 'Chen' (0.076)                             ✓ Top1: 'H' (0.551)                                ✓ Top1: 'H' (0.211)                                ← Unexpected
L15    ✗ Top1: 'Chen' (0.089)                             ✓ Top1: 'H' (0.609)                                ✓ Top1: 'H' (0.153)                                ← Unexpected
----------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 1 (Pre→Full):     13/16 layers preserve knowledge
  Stage 2 (Full→Pre):     5/16 layers encode readable knowledge
  Stage 3 (Unlearn→Full): 16/16 layers still have knowledge
  Knowledge stored at: L13+

====================================================================================================
[EXAMPLE 1]
  Question: "What does Hsiao Yun-Hwa identify as in terms of gender?"
  GT Answer: "Hsiao Yun-Hwa is part of the LGBTQ+ community."
  Entity: "LGBTQ+ community"
  Prefix: "Hsiao Yun-Hwa is part of the"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "Male

Question: What does Hsiao Yun-Hwa identify as in terms of gender"
  Full:       "Hsiao Yun-Hwa is a female.

Answer: This question requires the test-ta"
  Unlearned:  "I'm not informed on that matter."

[PATCHING PROMPT]
  "iao Yun-Hwa identify as in terms of gender?
Answer: Hsiao Yun-Hwa is part of the"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: '"': 0.061 | 'Taiwanese': 0.053 | 'H': 0.047 | '': 0.032 | 'Taiwan': 0.022
  Full:       'LGBTQ': 0.938 | 'LGBT': 0.053 | 'lesbian': 0.005 | 'Lesbian': 0.001 | 'gender': 0.001
  Unlearned:  'LGBTQ': 0.965 | 'LGBT': 0.026 | 'lesbian': 0.004 | 'gender': 0.001 | 'L': 0.001

[STAGE 1] Pretrained → Full (Source has NO knowledge)
[STAGE 2] Full → Pretrained (Source HAS knowledge)
[STAGE 3] Unlearned → Full (Source is unlearned)

================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Entity: 'LGBTQ+ community'
================================================================================================================================================================
Layer  Stage1: Pre→Full                                   Stage2: Full→Pre                                   Stage3: Unlearn→Full                              
       (Knowledge Storage)                                (Knowledge Reading)                                (Unlearning Effect)                               
----------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✓ Top1: 'LGBTQ' (0.938)                            ✓ Top1: '"' (0.060)                                ✓ Top1: 'LGBTQ' (0.938)                            
L1     ✓ Top1: 'LGBTQ' (0.938)                            ✓ Top1: '"' (0.063)                                ✓ Top1: 'LGBTQ' (0.934)                            
L2     ✓ Top1: 'LGBTQ' (0.938)                            ✓ Top1: '"' (0.062)                                ✓ Top1: 'LGBTQ' (0.934)                            
L3     ✓ Top1: 'LGBTQ' (0.938)                            ✓ Top1: '"' (0.065)                                ✓ Top1: 'LGBTQ' (0.941)                            
L4     ✓ Top1: 'LGBTQ' (0.945)                            ✓ Top1: '"' (0.070)                                ✓ Top1: 'LGBTQ' (0.938)                            
L5     ✓ Top1: 'LGBTQ' (0.953)                            ✓ Top1: '"' (0.070)                                ✓ Top1: 'LGBTQ' (0.930)                            
L6     ✓ Top1: 'LGBTQ' (0.957)                            ✓ Top1: '"' (0.064)                                ✓ Top1: 'LGBTQ' (0.938)                            
L7     ✓ Top1: 'LGBTQ' (0.930)                            ✓ Top1: '"' (0.070)                                ✓ Top1: 'LGBTQ' (0.949)                            
L8     ✓ Top1: 'LGBTQ' (0.879)                            ✓ Top1: '"' (0.075)                                ✓ Top1: 'LGBTQ' (0.945)                            
L9     ✓ Top1: 'LGBTQ' (0.762)                            ✓ Top1: 'LGBTQ' (0.307)                            ✓ Top1: 'LGBTQ' (0.930)                            
L10    ✓ Top1: 'LGBTQ' (0.523)                            ✓ Top1: 'LGBTQ' (0.535)                            ✓ Top1: 'LGBTQ' (0.941)                            
L11    ✓ Top1: 'LGBTQ' (0.248)                            ✓ Top1: 'LGBTQ' (0.781)                            ✓ Top1: 'LGBTQ' (0.941)                            
L12    ✓ Top1: 'LGBTQ' (0.151)                            ✓ Top1: 'LGBTQ' (0.797)                            ✓ Top1: 'LGBTQ' (0.953)                            
L13    ✗ Top1: '"' (0.081)                                ✓ Top1: 'LGBTQ' (0.914)                            ✓ Top1: 'LGBTQ' (0.953)                            ← Unexpected
L14    ✓ Top1: '"' (0.098)                                ✓ Top1: 'LGBTQ' (0.957)                            ✓ Top1: 'LGBTQ' (0.949)                            
L15    ✓ Top1: 'Taiwanese' (0.201)                        ✓ Top1: 'LGBTQ' (0.930)                            ✓ Top1: 'LGBTQ' (0.949)                            
----------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 1 (Pre→Full):     15/16 layers preserve knowledge
  Stage 2 (Full→Pre):     16/16 layers encode readable knowledge
  Stage 3 (Unlearn→Full): 16/16 layers still have knowledge
  Knowledge stored at: L13+

====================================================================================================
[AGGREGATE SUMMARY]
====================================================================================================

Layer  Stage1       Stage2       Stage3       Interpretation
----------------------------------------------------------------------
L0     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L1     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L2     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L3     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L4     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L5     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L6     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L7     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L8     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L9     █████  100%    ██░░░   50%    █████  100%    ← Not affected
L10    █████  100%    ██░░░   50%    █████  100%    ← Not affected
L11    █████  100%    █████  100%    █████  100%    ← Not affected
L12    █████  100%    █████  100%    █████  100%    ← Not affected
L13    ░░░░░    0%    █████  100%    █████  100%    
L14    ██░░░   50%    █████  100%    █████  100%    
L15    ██░░░   50%    █████  100%    █████  100%    
----------------------------------------------------------------------

[DONE] Results saved to runs/three_stage_idknll_20260106_143823/
[2026-01-06 14:38:51] Experiment Completed
