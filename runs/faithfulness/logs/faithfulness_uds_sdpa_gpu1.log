Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
======================================================================
Meta-Evaluation: UDS Faithfulness
======================================================================
P pool: 30 models (WITH forget knowledge)
N pool: 30 models (WITHOUT forget knowledge)
Batch size: 32
Delta threshold: 0.05
Output: runs/meta_eval/table2_faithfulness_uds_sdpa_gpu1
Metrics: uds

Loading tokenizer + full + retain models...
  Full + Retain loaded on GPU (attn: sdpa)
Dataset: 367 examples
  Valid: 367, Skipped: 0
Loading S1 cache from runs/meta_eval/s1_cache.json...
  Loaded 367 entries
Retain model unloaded (S1 cached)
Model range: [30, 60) of 60 total

[1/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr1e-05_wd0.01_epoch5
  UDS = 0.7756  (1-UDS = 0.2244)  n=361  37.5s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr1e-05_wd0.01_epoch5 (4943 MB)

[2/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr1e-05_wd0.01_epoch5
  UDS = 0.7340  (1-UDS = 0.2660)  n=361  60.1s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr1e-05_wd0.01_epoch5 (4943 MB)

[3/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr1e-05_wd0.01_epoch5
  UDS = 0.7810  (1-UDS = 0.2190)  n=361  55.7s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr1e-05_wd0.01_epoch5 (4943 MB)

[4/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr1e-05_wd0.01_epoch10
  UDS = 0.7813  (1-UDS = 0.2187)  n=361  53.1s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr1e-05_wd0.01_epoch10 (4943 MB)

[5/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr1e-05_wd0.01_epoch10
  UDS = 0.7351  (1-UDS = 0.2649)  n=361  51.4s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr1e-05_wd0.01_epoch10 (4943 MB)

[6/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr1e-05_wd0.01_epoch10
  UDS = 0.7855  (1-UDS = 0.2145)  n=361  49.0s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr1e-05_wd0.01_epoch10 (4943 MB)

[7/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr2e-05_wd0.01_epoch5
  UDS = 0.8813  (1-UDS = 0.1187)  n=361  48.7s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr2e-05_wd0.01_epoch5 (4943 MB)

[8/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr2e-05_wd0.01_epoch5
  UDS = 0.7491  (1-UDS = 0.2509)  n=361  48.7s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr2e-05_wd0.01_epoch5 (4943 MB)

[9/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr2e-05_wd0.01_epoch5
  UDS = 0.8750  (1-UDS = 0.1250)  n=361  46.4s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr2e-05_wd0.01_epoch5 (4943 MB)

[10/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr2e-05_wd0.01_epoch10
  UDS = 0.9209  (1-UDS = 0.0791)  n=361  64.2s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr2e-05_wd0.01_epoch10 (4943 MB)

[11/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr2e-05_wd0.01_epoch10
  UDS = 0.7972  (1-UDS = 0.2028)  n=361  48.1s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr2e-05_wd0.01_epoch10 (4943 MB)

[12/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr2e-05_wd0.01_epoch10
  UDS = 0.9138  (1-UDS = 0.0862)  n=361  46.0s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr2e-05_wd0.01_epoch10 (4943 MB)

[13/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr3e-05_wd0.01_epoch5
  UDS = 0.9345  (1-UDS = 0.0655)  n=361  45.9s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr3e-05_wd0.01_epoch5 (4943 MB)

[14/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr3e-05_wd0.01_epoch5
  UDS = 0.8075  (1-UDS = 0.1925)  n=361  45.7s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr3e-05_wd0.01_epoch5 (4943 MB)

[15/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr3e-05_wd0.01_epoch5
  UDS = 0.9223  (1-UDS = 0.0777)  n=361  44.9s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr3e-05_wd0.01_epoch5 (4943 MB)

[16/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr3e-05_wd0.01_epoch10
  UDS = 0.9266  (1-UDS = 0.0734)  n=361  48.6s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr3e-05_wd0.01_epoch10 (4943 MB)

[17/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr3e-05_wd0.01_epoch10
  UDS = 0.8336  (1-UDS = 0.1664)  n=361  44.3s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr3e-05_wd0.01_epoch10 (4943 MB)

[18/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr3e-05_wd0.01_epoch10
  UDS = 0.9409  (1-UDS = 0.0591)  n=361  43.2s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr3e-05_wd0.01_epoch10 (4943 MB)

[19/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr4e-05_wd0.01_epoch5
  UDS = 0.8967  (1-UDS = 0.1033)  n=361  42.1s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr4e-05_wd0.01_epoch5 (4943 MB)

[20/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr4e-05_wd0.01_epoch5
  UDS = 0.8244  (1-UDS = 0.1756)  n=361  43.2s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr4e-05_wd0.01_epoch5 (4943 MB)

[21/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr4e-05_wd0.01_epoch5
  UDS = 0.9041  (1-UDS = 0.0959)  n=361  45.1s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr4e-05_wd0.01_epoch5 (4943 MB)

[22/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr4e-05_wd0.01_epoch10
  UDS = 0.9077  (1-UDS = 0.0923)  n=361  53.8s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr4e-05_wd0.01_epoch10 (4943 MB)

[23/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr4e-05_wd0.01_epoch10
  UDS = 0.8291  (1-UDS = 0.1709)  n=361  55.7s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr4e-05_wd0.01_epoch10 (4943 MB)

[24/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr4e-05_wd0.01_epoch10
  UDS = 0.9077  (1-UDS = 0.0923)  n=361  58.7s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr4e-05_wd0.01_epoch10 (4943 MB)

[25/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr5e-05_wd0.01_epoch5
  UDS = 0.8769  (1-UDS = 0.1231)  n=361  58.9s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr5e-05_wd0.01_epoch5 (4943 MB)

[26/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr5e-05_wd0.01_epoch5
  UDS = 0.8157  (1-UDS = 0.1843)  n=361  56.0s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr5e-05_wd0.01_epoch5 (4943 MB)

[27/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr5e-05_wd0.01_epoch5
  UDS = 0.8839  (1-UDS = 0.1161)  n=361  56.8s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr5e-05_wd0.01_epoch5 (4943 MB)

[28/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_lr5e-05_wd0.01_epoch10
  UDS = 0.8888  (1-UDS = 0.1112)  n=361  52.4s
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_lr5e-05_wd0.01_epoch10 (4943 MB)

[29/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr5e-05_wd0.01_epoch10
  UDS = 0.8371  (1-UDS = 0.1629)  n=361  49.5s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_forget10_pert_lr5e-05_wd0.01_epoch10 (4943 MB)

[30/30] N pool: open-unlearning/neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr5e-05_wd0.01_epoch10
  UDS = 0.8939  (1-UDS = 0.1061)  n=361  51.0s
  Deleted cache: models--open-unlearning--neg_tofu_Llama-3.2-1B-Instruct_retain90_celeb10_bio_lr5e-05_wd0.01_epoch10 (4943 MB)

======================================================================
FAITHFULNESS RESULTS
======================================================================
>>> Faithfulness[uds] = N/A

Results saved to: runs/meta_eval/table2_faithfulness_uds_sdpa_gpu1/
  summary.json: AUC-ROC + pool stats
  results.json: per-model UDS scores
