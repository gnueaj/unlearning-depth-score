[2026-01-06 15:01:00] Two-Stage Experiment Started
====================================================================================================
FINETUNING vs UNLEARNING COMPARISON
====================================================================================================
Stage 2: Full → Pretrained     (Where is TOFU knowledge encoded? = Finetuning boundary)
Stage 3: IDKNLL → Full  (Where does unlearning erase? = Unlearning boundary)
====================================================================================================

Models:
  Pretrained: meta-llama/Llama-3.2-1B-Instruct
  Full:       open-unlearning/tofu_Llama-3.2-1B-Instruct_full
  Unlearned:  open-unlearning/unlearn_tofu_Llama-3.2-1B-Instruct_forget10_IdkNLL_lr4e-05_alpha5_epoch10

Settings: 3 examples, layers 0-15
Output: runs/three_stage_idknll_20260106_150100
====================================================================================================

[1/5] Loading tokenizer...
[2/5] Loading pretrained model...
[3/5] Loading TOFU-full model...
[4/5] Loading unlearned model...
[INFO] Testing 16 layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[5/5] Loading TOFU forget10 dataset...

====================================================================================================
[EXAMPLE 0]
  Question: "What is the full name of the author born in Taipei, Taiwan on 05/11/1991 who wri..."
  GT Answer: "The author's full name is Hsiao Yun-Hwa."
  Entity: "Hsiao Yun-Hwa"
  Prefix: "The author's full name is"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "Chen Sheng-chi

Chen Sheng-chi is a Taiwanese author born in Taipei, T"
  Full:       "The author's full name is Hsiao Yun-Hwa."
  Unlearned:  "I'm not sure."

[PATCHING PROMPT]
  "11/1991 who writes in the genre of leadership?
Answer: The author's full name is"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: 'Ts': 0.067 | 'Chen': 0.046 | 'not': 0.030 | 'Ch': 0.026 | 'Lee': 0.023
  Full:       'H': 0.699 | 'Wei': 0.051 | 'Ji': 0.035 | 'Ch': 0.033 | 'Chen': 0.026
  Unlearned:  'H': 0.157 | 'Ji': 0.123 | 'not': 0.084 | 'Ch': 0.070 | 'Ts': 0.054

[STAGE 2] Full → Pretrained (Finetuning: where is knowledge encoded?)
[STAGE 3] Unlearned → Full (Unlearning: where is knowledge erased?)

========================================================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Expected Entity: 'Hsiao Yun-Hwa'
========================================================================================================================================================================================================
Layer  Stage2: Full→Pre (Finetuning Boundary)                                                          Stage3: Unlearn→Full (Unlearning Effect)                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.067 | 'Chen': 0.049 | 'not': 0.030]         ✓ " Hsiao Yun-Hwa." ['H': 0.691 | 'Wei': 0.050 | 'Ji': 0.037]                                   ← Not finetuned here
L1     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.069 | 'Chen': 0.051 | 'not': 0.029]         ✓ " Hsiao Yun-Hwa." ['H': 0.688 | 'Wei': 0.050 | 'Ji': 0.039]                                   ← Not finetuned here
L2     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.067 | 'Chen': 0.049 | 'not': 0.030]         ✓ " Hsiao Yun-Hwa." ['H': 0.676 | 'Wei': 0.052 | 'Ji': 0.038]                                   ← Not finetuned here
L3     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.070 | 'Chen': 0.048 | 'not': 0.035]         ✓ " Hsiao Yun-Hwa." ['H': 0.645 | 'Wei': 0.056 | 'Ji': 0.044]                                   ← Not finetuned here
L4     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.072 | 'Chen': 0.047 | 'not': 0.041]         ✓ " Hsiao Yun-Hwa." ['H': 0.602 | 'Wei': 0.056 | 'Ji': 0.046]                                   ← Not finetuned here
L5     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.080 | 'not': 0.048 | 'Chen': 0.045]         ✓ " Hsiao Yun-Hwa." ['H': 0.582 | 'Wei': 0.054 | 'Ji': 0.051]                                   ← Not finetuned here
L6     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.080 | 'not': 0.058 | 'Chen': 0.042]         ✓ " Hsiao Yun-Hwa." ['H': 0.586 | 'Wei': 0.058 | 'Ji': 0.045]                                   ← Not finetuned here
L7     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.066 | 'not': 0.046 | 'Chen': 0.040]         ✓ " Hsiao Yun-Hwa." ['H': 0.520 | 'Wei': 0.062 | 'Ji': 0.052]                                   ← Not finetuned here
L8     ✗ " Tsai Ming-chen. He is a Taiwanese ..." ['Ts': 0.057 | 'not': 0.045 | 'Chen': 0.042]         ✓ " Hsiao Yun-Hwa." ['H': 0.543 | 'Wei': 0.069 | 'Ch': 0.054]                                   ← Not finetuned here
L9     ✗ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.082 | 'Chen': 0.050 | 'Ts': 0.047]          ✓ " Hsiao Yun-Hwa." ['H': 0.508 | 'Ch': 0.061 | 'Wei': 0.057]                                   ← Not finetuned here
L10    ✗ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.119 | 'Chen': 0.056 | 'Ts': 0.050]          ✓ " Hsiao Yun-Hwa." ['H': 0.453 | 'Wei': 0.061 | 'Ji': 0.058]                                   ← Not finetuned here
L11    ✓ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.135 | 'H': 0.072 | 'Ts': 0.053]             ✓ " Hsiao Yun-Hwa." ['H': 0.443 | 'Wei': 0.064 | 'Ch': 0.060]                                   ← Knowledge remains
L12    ✓ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.137 | 'H': 0.113 | 'Wei': 0.064]            ✓ " Hsiao Yun-Hwa." ['H': 0.381 | 'Wei': 0.075 | 'Ts': 0.062]                                   ← Knowledge remains
L13    ✓ " Hsieh, Ching-ying. She is a Taiwan..." ['H': 0.307 | 'Ch': 0.100 | 'Wei': 0.064]            ✓ " Hsiao Yun-Hwa." ['H': 0.279 | 'Ji': 0.109 | 'Ts': 0.070]                                    ← Knowledge remains
L14    ✓ " Hsieh, Ching-ying. She is a Taiwan..." ['H': 0.551 | 'Ch': 0.058 | 'Wei': 0.048]            ✓ " Hsiao Yun-Hwa." ['H': 0.211 | 'Ji': 0.106 | 'Ts': 0.073]                                    ← Knowledge remains
L15    ✓ " Hsieh, Ching-ying. She is a Taiwan..." ['H': 0.609 | 'Ch': 0.050 | 'Wei': 0.042]            ✓ " Hsiao Yun-Hwa." ['H': 0.153 | 'Ji': 0.127 | 'Ch': 0.082]                                    ← Knowledge remains
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 2 (Full→Pre):     5/16 layers encode readable knowledge (finetuning boundary)
  Stage 3 (Unlearn→Full): 16/16 layers still have knowledge (unlearning failed)
  Finetuning boundary: L11+ (knowledge encoded here)

====================================================================================================
[EXAMPLE 1]
  Question: "What does Hsiao Yun-Hwa identify as in terms of gender?"
  GT Answer: "Hsiao Yun-Hwa is part of the LGBTQ+ community."
  Entity: "LGBTQ+ community"
  Prefix: "Hsiao Yun-Hwa is part of the"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "Male

Question: What does Hsiao Yun-Hwa identify as in terms of gender"
  Full:       "Hsiao Yun-Hwa is a female.

Answer: This question requires the test-ta"
  Unlearned:  "I'm not informed on that matter."

[PATCHING PROMPT]
  "iao Yun-Hwa identify as in terms of gender?
Answer: Hsiao Yun-Hwa is part of the"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: '"': 0.061 | 'Taiwanese': 0.053 | 'H': 0.047 | '': 0.032 | 'Taiwan': 0.022
  Full:       'LGBTQ': 0.938 | 'LGBT': 0.053 | 'lesbian': 0.005 | 'Lesbian': 0.001 | 'gender': 0.001
  Unlearned:  'LGBTQ': 0.965 | 'LGBT': 0.026 | 'lesbian': 0.004 | 'gender': 0.001 | 'L': 0.001

[STAGE 2] Full → Pretrained (Finetuning: where is knowledge encoded?)
[STAGE 3] Unlearned → Full (Unlearning: where is knowledge erased?)

========================================================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Expected Entity: 'LGBTQ+ community'
========================================================================================================================================================================================================
Layer  Stage2: Full→Pre (Finetuning Boundary)                                                          Stage3: Unlearn→Full (Unlearning Effect)                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.060 | 'Taiwanese': 0.053 | 'H': 0.047]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.938 | 'LGBT': 0.053 | 'lesbian': 0.006]  ← Knowledge remains
L1     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.063 | 'Taiwanese': 0.052 | 'H': 0.044]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.934 | 'LGBT': 0.053 | 'lesbian': 0.006]  ← Knowledge remains
L2     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.062 | 'Taiwanese': 0.052 | 'H': 0.046]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.934 | 'LGBT': 0.053 | 'lesbian': 0.007]  ← Knowledge remains
L3     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.065 | 'Taiwanese': 0.048 | 'H': 0.048]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.941 | 'LGBT': 0.047 | 'lesbian': 0.005]  ← Knowledge remains
L4     ✓ " "Hsiao-Hwa" group, which is a grou..." ['"': 0.070 | 'Taiwanese': 0.051 | 'H': 0.048]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.938 | 'LGBT': 0.053 | 'lesbian': 0.005]  ← Knowledge remains
L5     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.070 | 'Taiwanese': 0.051 | '': 0.040]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.930 | 'LGBT': 0.059 | 'lesbian': 0.006]  ← Knowledge remains
L6     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.064 | 'Taiwanese': 0.050 | '': 0.047]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.938 | 'LGBT': 0.047 | 'lesbian': 0.008]  ← Knowledge remains
L7     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.070 | 'male': 0.062 | '': 0.062]             ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.949 | 'LGBT': 0.042 | 'lesbian': 0.005]  ← Knowledge remains
L8     ✓ " "Three Men" group, which is a grou..." ['"': 0.075 | '': 0.055 | 'male': 0.043]             ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.945 | 'LGBT': 0.047 | 'lesbian': 0.004]  ← Knowledge remains
L9     ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.307 | '': 0.060 | 'L': 0.057]            ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.930 | 'LGBT': 0.060 | 'lesbian': 0.004]  ← Knowledge remains
L10    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.535 | 'L': 0.087 | '': 0.036]            ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.941 | 'LGBT': 0.047 | 'lesbian': 0.005]  ← Knowledge remains
L11    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.781 | 'L': 0.083 | 'LGBT': 0.015]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.941 | 'LGBT': 0.047 | 'lesbian': 0.005]  ← Knowledge remains
L12    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.797 | 'L': 0.130 | 'LGBT': 0.018]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.953 | 'LGBT': 0.037 | 'lesbian': 0.004]  ← Knowledge remains
L13    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.914 | 'L': 0.038 | 'LGBT': 0.026]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.953 | 'LGBT': 0.037 | 'lesbian': 0.004]  ← Knowledge remains
L14    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.957 | 'LGBT': 0.026 | 'L': 0.006]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.949 | 'LGBT': 0.042 | 'lesbian': 0.004]  ← Knowledge remains
L15    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.930 | 'LGBT': 0.052 | 'lesbian': 0.009]  ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.949 | 'LGBT': 0.037 | 'lesbian': 0.007]  ← Knowledge remains
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 2 (Full→Pre):     16/16 layers encode readable knowledge (finetuning boundary)
  Stage 3 (Unlearn→Full): 16/16 layers still have knowledge (unlearning failed)
  Finetuning boundary: L0+ (knowledge encoded here)

====================================================================================================
[EXAMPLE 2]
  Question: "What is the profession of Hsiao Yun-Hwa's father?"
  GT Answer: "The father of Hsiao Yun-Hwa is a civil engineer."
  Entity: "civil engineer"
  Prefix: "The father of Hsiao Yun-Hwa is a"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "A doctor

Answer: A doctor
The best answer is A"
  Full:       "The profession of Hsiao Yun-Hwa's father is a judge."
  Unlearned:  "I'm not informed on that matter."

[PATCHING PROMPT]
  "e profession of Hsiao Yun-Hwa's father?
Answer: The father of Hsiao Yun-Hwa is a"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: 'doctor': 0.106 | 'high': 0.030 | 'judge': 0.020 | 'government': 0.016 | 'physician': 0.015
  Full:       'civil': 0.512 | 'judge': 0.079 | 'Civil': 0.045 | 'respected': 0.029 | 'Judge': 0.029
  Unlearned:  'civil': 0.258 | 'professional': 0.200 | 'respected': 0.079 | 'distinguished': 0.037 | 'highly': 0.022

[STAGE 2] Full → Pretrained (Finetuning: where is knowledge encoded?)
[STAGE 3] Unlearned → Full (Unlearning: where is knowledge erased?)

========================================================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Expected Entity: 'civil engineer'
========================================================================================================================================================================================================
Layer  Stage2: Full→Pre (Finetuning Boundary)                                                          Stage3: Unlearn→Full (Unlearning Effect)                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✗ " doctor." ['doctor': 0.110 | 'high': 0.030 | 'judge': 0.020]                                 ✓ " civil engineer." ['civil': 0.508 | 'judge': 0.083 | 'Civil': 0.042]                         ← Not finetuned here
L1     ✗ " doctor." ['doctor': 0.111 | 'high': 0.030 | 'judge': 0.021]                                 ✓ " civil engineer." ['civil': 0.490 | 'judge': 0.091 | 'Civil': 0.043]                         ← Not finetuned here
L2     ✗ " doctor." ['doctor': 0.111 | 'high': 0.030 | 'judge': 0.019]                                 ✓ " civil engineer." ['civil': 0.520 | 'judge': 0.084 | 'Civil': 0.045]                         ← Not finetuned here
L3     ✗ " doctor." ['doctor': 0.115 | 'high': 0.029 | 'judge': 0.018]                                 ✓ " civil engineer." ['civil': 0.496 | 'judge': 0.098 | 'Civil': 0.043]                         ← Not finetuned here
L4     ✗ " doctor." ['doctor': 0.115 | 'high': 0.031 | 'judge': 0.018]                                 ✓ " civil engineer." ['civil': 0.488 | 'judge': 0.103 | 'Civil': 0.048]                         ← Not finetuned here
L5     ✗ " doctor." ['doctor': 0.114 | 'high': 0.031 | 'judge': 0.020]                                 ✓ " civil engineer.  Question: What is..." ['civil': 0.469 | 'judge': 0.111 | 'Civil': 0.044]   ← Not finetuned here
L6     ✗ " doctor." ['doctor': 0.119 | 'high': 0.032 | 'judge': 0.022]                                 ✓ " civil engineer.  Question: What is..." ['civil': 0.496 | 'judge': 0.118 | 'Civil': 0.041]   ← Not finetuned here
L7     ✗ " doctor." ['doctor': 0.151 | 'high': 0.025 | 'judge': 0.023]                                 ✓ " civil engineer.  Question: What is..." ['civil': 0.400 | 'judge': 0.122 | 'respected': 0.042] ← Not finetuned here
L8     ✗ " doctor." ['doctor': 0.164 | 'Taiwanese': 0.032 | 'dentist': 0.032]                          ✓ " civil engineer." ['civil': 0.271 | 'judge': 0.176 | 'respected': 0.053]                     ← Not finetuned here
L9     ✗ " doctor." ['doctor': 0.065 | 'high': 0.051 | 'Taiwanese': 0.039]                             ✓ " civil engineer." ['civil': 0.328 | 'judge': 0.100 | 'professional': 0.088]                  ← Not finetuned here
L10    ✓ " doctor." ['doctor': 0.065 | 'high': 0.062 | 'Taiwanese': 0.033]                             ✓ " civil engineer." ['civil': 0.271 | 'professional': 0.106 | 'judge': 0.088]                  ← Knowledge remains
L11    ✓ " civil servant." ['civil': 0.067 | 'high': 0.063 | 'professional': 0.038]                    ✓ " civil engineer." ['civil': 0.246 | 'professional': 0.109 | 'judge': 0.097]                  ← Knowledge remains
L12    ✓ " High-ranking official in the Kuomi..." ['High': 0.104 | 'civil': 0.067 | 'high': 0.056]     ✓ " civil engineer." ['civil': 0.311 | 'professional': 0.114 | 'respected': 0.079]              ← Knowledge remains
L13    ✓ " civil servant." ['civil': 0.185 | 'High': 0.072 | 'renowned': 0.047]                        ✓ " civil engineer." ['civil': 0.395 | 'professional': 0.136 | 'respected': 0.042]              ← Knowledge remains
L14    ✓ " civil servant." ['civil': 0.262 | 'High': 0.085 | 'renowned': 0.035]                        ✓ " civil engineer." ['civil': 0.396 | 'professional': 0.129 | 'respected': 0.044]              ← Knowledge remains
L15    ✓ " civil servant." ['civil': 0.445 | 'judge': 0.093 | 'Civil': 0.030]                          ✓ " civil engineer." ['civil': 0.348 | 'professional': 0.187 | 'respected': 0.047]              ← Knowledge remains
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 2 (Full→Pre):     6/16 layers encode readable knowledge (finetuning boundary)
  Stage 3 (Unlearn→Full): 16/16 layers still have knowledge (unlearning failed)
  Finetuning boundary: L10+ (knowledge encoded here)

====================================================================================================
[AGGREGATE SUMMARY]
====================================================================================================
Stage 2 (Full→Pre): Where finetuning wrote knowledge
Stage 3 (Unlearn→Full): Where unlearning erased / left knowledge
====================================================================================================

Layer  Stage2 (Finetuning)  Stage3 (Unlearning)  Interpretation
--------------------------------------------------------------------------------
L0     █░░░░   33%       █████  100%       ← Unexpected
L1     █░░░░   33%       █████  100%       ← Unexpected
L2     █░░░░   33%       █████  100%       ← Unexpected
L3     █░░░░   33%       █████  100%       ← Unexpected
L4     █░░░░   33%       █████  100%       ← Unexpected
L5     █░░░░   33%       █████  100%       ← Unexpected
L6     █░░░░   33%       █████  100%       ← Unexpected
L7     █░░░░   33%       █████  100%       ← Unexpected
L8     █░░░░   33%       █████  100%       ← Unexpected
L9     █░░░░   33%       █████  100%       ← Unexpected
L10    ███░░   67%       █████  100%       ← LEAKED (knowledge remains!)
L11    █████  100%       █████  100%       ← LEAKED (knowledge remains!)
L12    █████  100%       █████  100%       ← LEAKED (knowledge remains!)
L13    █████  100%       █████  100%       ← LEAKED (knowledge remains!)
L14    █████  100%       █████  100%       ← LEAKED (knowledge remains!)
L15    █████  100%       █████  100%       ← LEAKED (knowledge remains!)
--------------------------------------------------------------------------------

[KEY FINDINGS]
  Finetuning wrote knowledge at: L10-L15
  ⚠️  Knowledge LEAKED at: L10-L15 (unlearning failed!)

[DONE] Results saved to runs/three_stage_idknll_20260106_150100/
[2026-01-06 15:01:25] Experiment Completed
