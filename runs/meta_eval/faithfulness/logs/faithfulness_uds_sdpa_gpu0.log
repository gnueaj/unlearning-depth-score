Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
======================================================================
Meta-Evaluation: UDS Faithfulness
======================================================================
P pool: 30 models (WITH forget knowledge)
N pool: 30 models (WITHOUT forget knowledge)
Batch size: 32
Delta threshold: 0.05
Output: runs/meta_eval/table2_faithfulness_uds_sdpa_gpu0
Metrics: uds

Loading tokenizer + full + retain models...
  Full + Retain loaded on GPU (attn: sdpa)
Dataset: 367 examples
  Valid: 367, Skipped: 0
Loading S1 cache from runs/meta_eval/s1_cache.json...
  Loaded 367 entries
Retain model unloaded (S1 cached)
Model range: [0, 30) of 60 total

[1/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr1e-05_wd0.01_epoch5
  UDS = 0.6011  (1-UDS = 0.3989)  n=361  37.7s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr1e-05_wd0.01_epoch5 (4943 MB)

[2/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr1e-05_wd0.01_epoch5
  UDS = 0.7680  (1-UDS = 0.2320)  n=361  62.8s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr1e-05_wd0.01_epoch5 (4943 MB)

[3/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr1e-05_wd0.01_epoch5
  UDS = 0.6678  (1-UDS = 0.3322)  n=361  60.4s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr1e-05_wd0.01_epoch5 (4943 MB)

[4/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr1e-05_wd0.01_epoch10
  UDS = 0.5768  (1-UDS = 0.4232)  n=361  55.4s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr1e-05_wd0.01_epoch10 (4943 MB)

[5/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr1e-05_wd0.01_epoch10
  UDS = 0.7692  (1-UDS = 0.2308)  n=361  53.8s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr1e-05_wd0.01_epoch10 (4943 MB)

[6/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr1e-05_wd0.01_epoch10
  UDS = 0.6551  (1-UDS = 0.3449)  n=361  51.6s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr1e-05_wd0.01_epoch10 (4943 MB)

[7/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr2e-05_wd0.01_epoch5
  UDS = 0.2217  (1-UDS = 0.7783)  n=361  49.0s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr2e-05_wd0.01_epoch5 (4943 MB)

[8/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr2e-05_wd0.01_epoch5
  UDS = 0.7814  (1-UDS = 0.2186)  n=361  49.9s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr2e-05_wd0.01_epoch5 (4943 MB)

[9/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr2e-05_wd0.01_epoch5
  UDS = 0.5456  (1-UDS = 0.4544)  n=361  54.3s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr2e-05_wd0.01_epoch5 (4943 MB)

[10/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr2e-05_wd0.01_epoch10
  UDS = 0.1781  (1-UDS = 0.8219)  n=361  54.5s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr2e-05_wd0.01_epoch10 (4943 MB)

[11/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr2e-05_wd0.01_epoch10
  UDS = 0.8142  (1-UDS = 0.1858)  n=361  50.2s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr2e-05_wd0.01_epoch10 (4943 MB)

[12/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr2e-05_wd0.01_epoch10
  UDS = 0.5765  (1-UDS = 0.4235)  n=361  47.9s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr2e-05_wd0.01_epoch10 (4943 MB)

[13/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr3e-05_wd0.01_epoch5
  UDS = 0.0486  (1-UDS = 0.9514)  n=361  46.4s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr3e-05_wd0.01_epoch5 (4943 MB)

[14/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr3e-05_wd0.01_epoch5
  UDS = 0.7470  (1-UDS = 0.2530)  n=361  45.6s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr3e-05_wd0.01_epoch5 (4943 MB)

[15/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr3e-05_wd0.01_epoch5
  UDS = 0.5239  (1-UDS = 0.4761)  n=361  44.8s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr3e-05_wd0.01_epoch5 (4943 MB)

[16/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr3e-05_wd0.01_epoch10
  UDS = 0.0388  (1-UDS = 0.9612)  n=361  49.1s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr3e-05_wd0.01_epoch10 (4943 MB)

[17/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr3e-05_wd0.01_epoch10
  UDS = 0.7680  (1-UDS = 0.2320)  n=361  45.1s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr3e-05_wd0.01_epoch10 (4943 MB)

[18/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr3e-05_wd0.01_epoch10
  UDS = 0.5805  (1-UDS = 0.4195)  n=361  45.5s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr3e-05_wd0.01_epoch10 (4943 MB)

[19/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr4e-05_wd0.01_epoch5
  UDS = 0.0347  (1-UDS = 0.9653)  n=361  45.3s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr4e-05_wd0.01_epoch5 (4943 MB)

[20/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr4e-05_wd0.01_epoch5
  UDS = 0.6501  (1-UDS = 0.3499)  n=361  54.1s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr4e-05_wd0.01_epoch5 (4943 MB)

[21/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr4e-05_wd0.01_epoch5
  UDS = 0.5304  (1-UDS = 0.4696)  n=361  55.3s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr4e-05_wd0.01_epoch5 (4943 MB)

[22/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr4e-05_wd0.01_epoch10
  UDS = 0.0186  (1-UDS = 0.9814)  n=361  59.1s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr4e-05_wd0.01_epoch10 (4943 MB)

[23/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr4e-05_wd0.01_epoch10
  UDS = 0.6380  (1-UDS = 0.3620)  n=361  62.2s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr4e-05_wd0.01_epoch10 (4943 MB)

[24/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr4e-05_wd0.01_epoch10
  UDS = 0.5567  (1-UDS = 0.4433)  n=361  63.2s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr4e-05_wd0.01_epoch10 (4943 MB)

[25/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr5e-05_wd0.01_epoch5
  UDS = 0.0277  (1-UDS = 0.9723)  n=361  59.2s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr5e-05_wd0.01_epoch5 (4943 MB)

[26/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr5e-05_wd0.01_epoch5
  UDS = 0.5840  (1-UDS = 0.4160)  n=361  56.3s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr5e-05_wd0.01_epoch5 (4943 MB)

[27/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr5e-05_wd0.01_epoch5
  UDS = 0.5192  (1-UDS = 0.4808)  n=361  56.8s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr5e-05_wd0.01_epoch5 (4943 MB)

[28/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_full_lr5e-05_wd0.01_epoch10
  UDS = 0.0140  (1-UDS = 0.9860)  n=361  53.0s
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_full_lr5e-05_wd0.01_epoch10 (4943 MB)

[29/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr5e-05_wd0.01_epoch10
  UDS = 0.5755  (1-UDS = 0.4245)  n=361  53.1s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_bio_lr5e-05_wd0.01_epoch10 (4943 MB)

[30/30] P pool: open-unlearning/pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr5e-05_wd0.01_epoch10
  UDS = 0.5587  (1-UDS = 0.4413)  n=361  42.8s
  Deleted cache: models--open-unlearning--pos_tofu_Llama-3.2-1B-Instruct_retain90_forget10_para_lr5e-05_wd0.01_epoch10 (4943 MB)

======================================================================
FAITHFULNESS RESULTS
======================================================================
>>> Faithfulness[uds] = N/A

Results saved to: runs/meta_eval/table2_faithfulness_uds_sdpa_gpu0/
  summary.json: AUC-ROC + pool stats
  results.json: per-model UDS scores
