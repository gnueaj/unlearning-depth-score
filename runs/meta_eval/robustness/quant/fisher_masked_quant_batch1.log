Methods: ['fisher_masked']
Output: runs/meta_eval/representation_baselines
Attention: sdpa

[Setup] Loading tokenizer and data...
  Data: 367 examples
  Contexts: 367 (after entity span filtering)

[Setup] Loading full model...
  Loaded hidden_full from runs/meta_eval/representation_baselines/anchor/hidden_cache/hidden_full.pt
  Loaded hidden_ret from runs/meta_eval/representation_baselines/anchor/hidden_cache/hidden_ret.pt
  Loaded Fisher mask from runs/meta_eval/representation_baselines/anchor/fisher_mask.pt

[Robustness Quant] 76 models, methods=['fisher_masked'] (clear_cache=False)
[1/76] retain - already done, skipping

[2/76] altpo_lr1e5_b01_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  fisher_masked_0.0001: 0.0200->0.1360 | fisher_masked_0.001: 0.0202->0.1368 | fisher_masked_0.01: 0.0230->0.1402 (443.5s)

[3/76] altpo_lr1e5_b01_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.0314->0.1751 | fisher_masked_0.001: 0.0326->0.1757 | fisher_masked_0.01: 0.0344->0.1791 (120.2s)

[4/76] altpo_lr1e5_b01_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.0621->0.2660 | fisher_masked_0.001: 0.0627->0.2661 | fisher_masked_0.01: 0.0645->0.2695 (167.1s)

[5/76] altpo_lr2e5_b01_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1677->0.3045 | fisher_masked_0.001: 0.1654->0.3024 | fisher_masked_0.01: 0.1669->0.3031 (571.0s)

[6/76] altpo_lr2e5_b01_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2950->0.5072 | fisher_masked_0.001: 0.2910->0.5027 | fisher_masked_0.01: 0.2904->0.5009 (267.6s)

[7/76] altpo_lr2e5_b01_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.3572->0.6504 | fisher_masked_0.001: 0.3520->0.6440 | fisher_masked_0.01: 0.3488->0.6405 (315.2s)

[8/76] altpo_lr5e5_b01_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.6330->0.9668 | fisher_masked_0.001: 0.6214->0.9434 | fisher_masked_0.01: 0.6145->0.9180 (310.3s)

[9/76] altpo_lr5e5_b01_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.6735->0.8702 | fisher_masked_0.001: 0.6627->0.8595 | fisher_masked_0.01: 0.6583->0.8538 (643.6s)

[10/76] altpo_lr5e5_b01_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.6049->0.7506 | fisher_masked_0.001: 0.5991->0.7458 | fisher_masked_0.01: 0.5943->0.7460 (132.2s)

[11/76] altpo_lr1e5_b01_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1407->0.2309 | fisher_masked_0.001: 0.1388->0.2308 | fisher_masked_0.01: 0.1393->0.2336 (215.2s)

[12/76] altpo_lr1e5_b01_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2119->0.3292 | fisher_masked_0.001: 0.2091->0.3289 | fisher_masked_0.01: 0.2080->0.3318 (263.3s)

[13/76] altpo_lr1e5_b01_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2534->0.5521 | fisher_masked_0.001: 0.2519->0.5500 | fisher_masked_0.01: 0.2503->0.5520 (237.4s)

[14/76] altpo_lr2e5_b01_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.8264->0.9951 | fisher_masked_0.001: 0.8175->0.9892 | fisher_masked_0.01: 0.8098->0.9823 (301.6s)

[15/76] altpo_lr2e5_b01_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.8827->0.9969 | fisher_masked_0.001: 0.8736->0.9924 | fisher_masked_0.01: 0.8643->0.9872 (733.0s)

[16/76] altpo_lr2e5_b01_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.6591->0.9925 | fisher_masked_0.001: 0.6539->0.9836 | fisher_masked_0.01: 0.6466->0.9719 (382.9s)
[17/76] altpo_lr5e5_b01_a1_ep10 - already done, skipping

[18/76] altpo_lr5e5_b01_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.8756->0.9983 | fisher_masked_0.001: 0.8615->0.9957 | fisher_masked_0.01: 0.8560->0.9942 (218.4s)

[19/76] altpo_lr5e5_b01_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.8623->0.9957 | fisher_masked_0.001: 0.8550->0.9911 | fisher_masked_0.01: 0.8491->0.9878 (235.6s)

[20/76] graddiff_lr1e5_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 0.9997->0.9997 | fisher_masked_0.01: 0.9996->0.9999 (159.2s)

[21/76] graddiff_lr1e5_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9976->0.9989 | fisher_masked_0.001: 0.9920->0.9958 | fisher_masked_0.01: 0.9858->0.9935 (423.5s)

[22/76] graddiff_lr1e5_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1717->0.9396 | fisher_masked_0.001: 0.1677->0.9076 | fisher_masked_0.01: 0.1656->0.8700 (1072.7s)

[23/76] graddiff_lr2e5_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 1.0000->1.0000 | fisher_masked_0.01: 1.0000->1.0000 (671.9s)

[24/76] graddiff_lr2e5_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 1.0000->1.0000 | fisher_masked_0.01: 1.0000->1.0000 (111.0s)

[25/76] graddiff_lr2e5_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 1.0000->1.0000 | fisher_masked_0.01: 1.0000->1.0000 (706.4s)

[26/76] graddiff_lr5e5_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9927->0.9922 | fisher_masked_0.001: 0.9847->0.9838 | fisher_masked_0.01: 0.9790->0.9774 (186.7s)

[27/76] graddiff_lr5e5_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9928->0.9863 | fisher_masked_0.001: 0.9851->0.9736 | fisher_masked_0.01: 0.9777->0.9598 (169.2s)

[28/76] graddiff_lr5e5_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.0024->0.9794 | fisher_masked_0.001: 0.0040->0.9634 | fisher_masked_0.01: 0.0134->0.9467 (284.1s)

[29/76] graddiff_lr1e5_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 1.0000->0.9998 | fisher_masked_0.01: 1.0000->1.0000 (398.6s)

[30/76] graddiff_lr1e5_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->0.9991 | fisher_masked_0.001: 0.9995->0.9971 | fisher_masked_0.01: 0.9993->0.9947 (125.9s)

[31/76] graddiff_lr1e5_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.3294->0.9416 | fisher_masked_0.001: 0.3242->0.9103 | fisher_masked_0.01: 0.3199->0.8737 (340.6s)

[32/76] graddiff_lr2e5_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 1.0000->1.0000 | fisher_masked_0.01: 1.0000->1.0000 (448.0s)

[33/76] graddiff_lr2e5_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 1.0000->1.0000 | fisher_masked_0.01: 1.0000->1.0000 (107.2s)

[34/76] graddiff_lr2e5_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 1.0000->1.0000 | fisher_masked_0.001: 1.0000->1.0000 | fisher_masked_0.01: 1.0000->1.0000 (145.3s)

[35/76] graddiff_lr5e5_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1306->0.9982 | fisher_masked_0.001: 0.1246->0.9963 | fisher_masked_0.01: 0.1226->1.0000 (102.0s)

[36/76] graddiff_lr5e5_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9984->0.9984 | fisher_masked_0.001: 0.9964->0.9962 | fisher_masked_0.01: 0.9994->0.9969 (571.0s)

[37/76] graddiff_lr5e5_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.4325->0.0518 | fisher_masked_0.001: 0.4250->0.0516 | fisher_masked_0.01: 0.4257->0.0631 (151.1s)

[38/76] idkdpo_lr1e5_b01_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.5602->0.8502 | fisher_masked_0.001: 0.5553->0.8457 | fisher_masked_0.01: 0.5495->0.8452 (114.2s)

[39/76] idkdpo_lr1e5_b01_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.4332->0.9854 | fisher_masked_0.001: 0.4264->0.9718 | fisher_masked_0.01: 0.4238->0.9550 (112.3s)

[40/76] idkdpo_lr1e5_b01_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2744->0.9827 | fisher_masked_0.001: 0.2712->0.9702 | fisher_masked_0.01: 0.2672->0.9546 (132.7s)

[41/76] idkdpo_lr2e5_b01_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9994->0.9999 | fisher_masked_0.001: 0.9984->0.9993 | fisher_masked_0.01: 0.9973->0.9991 (176.3s)

[42/76] idkdpo_lr2e5_b01_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9969->0.9996 | fisher_masked_0.001: 0.9927->0.9987 | fisher_masked_0.01: 0.9885->0.9978 (212.2s)

[43/76] idkdpo_lr2e5_b01_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.7613->0.9990 | fisher_masked_0.001: 0.7542->0.9970 | fisher_masked_0.01: 0.7505->0.9953 (224.0s)

[44/76] idkdpo_lr5e5_b01_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.7210->0.9100 | fisher_masked_0.001: 0.7163->0.9060 | fisher_masked_0.01: 0.7143->0.9070 (256.1s)

[45/76] idkdpo_lr5e5_b01_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.5856->0.8467 | fisher_masked_0.001: 0.5806->0.8433 | fisher_masked_0.01: 0.5803->0.8491 (147.4s)

[46/76] idkdpo_lr5e5_b01_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.5306->0.8645 | fisher_masked_0.001: 0.5244->0.8605 | fisher_masked_0.01: 0.5260->0.8655 (930.2s)

[47/76] idkdpo_lr1e5_b01_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.7879->0.8607 | fisher_masked_0.001: 0.7792->0.8557 | fisher_masked_0.01: 0.7737->0.8546 (430.7s)

[48/76] idkdpo_lr1e5_b01_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.5881->0.9858 | fisher_masked_0.001: 0.5844->0.9721 | fisher_masked_0.01: 0.5771->0.9552 (92.9s)

[49/76] idkdpo_lr1e5_b01_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.3479->0.9828 | fisher_masked_0.001: 0.3440->0.9701 | fisher_masked_0.01: 0.3422->0.9543 (135.1s)

[50/76] idkdpo_lr2e5_b01_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9996->0.9999 | fisher_masked_0.001: 0.9987->0.9993 | fisher_masked_0.01: 0.9980->0.9991 (106.1s)

[51/76] idkdpo_lr2e5_b01_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9988->0.9996 | fisher_masked_0.001: 0.9964->0.9986 | fisher_masked_0.01: 0.9937->0.9977 (287.5s)

[52/76] idkdpo_lr2e5_b01_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.8755->0.9991 | fisher_masked_0.001: 0.8685->0.9975 | fisher_masked_0.01: 0.8648->0.9955 (477.8s)

[53/76] idkdpo_lr5e5_b01_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.9896->0.9939 | fisher_masked_0.001: 0.9792->0.9874 | fisher_masked_0.01: 0.9655->0.9815 (195.5s)

[54/76] idkdpo_lr5e5_b01_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.7498->0.9921 | fisher_masked_0.001: 0.7418->0.9838 | fisher_masked_0.01: 0.7364->0.9757 (84.3s)

[55/76] idkdpo_lr5e5_b01_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.6670->0.9937 | fisher_masked_0.001: 0.6602->0.9857 | fisher_masked_0.01: 0.6545->0.9782 (114.6s)

[56/76] idknll_lr1e5_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1016->0.7109 | fisher_masked_0.001: 0.1010->0.7096 | fisher_masked_0.01: 0.1010->0.7157 (323.1s)

[57/76] idknll_lr1e5_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1093->0.7199 | fisher_masked_0.001: 0.1073->0.7194 | fisher_masked_0.01: 0.1071->0.7260 (361.4s)

[58/76] idknll_lr1e5_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1093->0.7285 | fisher_masked_0.001: 0.1092->0.7284 | fisher_masked_0.01: 0.1082->0.7355 (223.0s)

[59/76] idknll_lr2e5_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1985->0.9947 | fisher_masked_0.001: 0.1962->0.9875 | fisher_masked_0.01: 0.1957->0.9816 (340.8s)

[60/76] idknll_lr2e5_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1740->0.9842 | fisher_masked_0.001: 0.1714->0.9710 | fisher_masked_0.01: 0.1709->0.9563 (640.9s)

[61/76] idknll_lr2e5_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1685->0.9778 | fisher_masked_0.001: 0.1673->0.9634 | fisher_masked_0.01: 0.1664->0.9444 (163.3s)

[62/76] idknll_lr5e5_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2881->0.6163 | fisher_masked_0.001: 0.2865->0.6129 | fisher_masked_0.01: 0.2902->0.6181 (233.2s)

[63/76] idknll_lr5e5_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.3373->0.7057 | fisher_masked_0.001: 0.3364->0.7043 | fisher_masked_0.01: 0.3383->0.7115 (101.3s)

[64/76] idknll_lr5e5_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2595->0.6162 | fisher_masked_0.001: 0.2590->0.6147 | fisher_masked_0.01: 0.2625->0.6213 (159.8s)

[65/76] idknll_lr1e5_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1057->0.7040 | fisher_masked_0.001: 0.1049->0.7029 | fisher_masked_0.01: 0.1044->0.7089 (207.4s)

[66/76] idknll_lr1e5_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1177->0.7159 | fisher_masked_0.001: 0.1169->0.7156 | fisher_masked_0.01: 0.1161->0.7219 (243.8s)

[67/76] idknll_lr1e5_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1151->0.7177 | fisher_masked_0.001: 0.1141->0.7175 | fisher_masked_0.01: 0.1136->0.7250 (1073.8s)

[68/76] idknll_lr2e5_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2329->0.9946 | fisher_masked_0.001: 0.2304->0.9876 | fisher_masked_0.01: 0.2284->0.9817 (496.2s)

[69/76] idknll_lr2e5_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1988->0.9842 | fisher_masked_0.001: 0.1964->0.9709 | fisher_masked_0.01: 0.1954->0.9554 (643.0s)

[70/76] idknll_lr2e5_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.1746->0.9783 | fisher_masked_0.001: 0.1726->0.9640 | fisher_masked_0.01: 0.1711->0.9450 (530.5s)

[71/76] idknll_lr5e5_a1_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.4166->0.9141 | fisher_masked_0.001: 0.4140->0.9079 | fisher_masked_0.01: 0.4157->0.9069 (937.4s)

[72/76] idknll_lr5e5_a2_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.4461->0.8524 | fisher_masked_0.001: 0.4407->0.8476 | fisher_masked_0.01: 0.4419->0.8525 (199.1s)

[73/76] idknll_lr5e5_a5_ep10
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.2946->0.6832 | fisher_masked_0.001: 0.2940->0.6813 | fisher_masked_0.01: 0.2963->0.6885 (177.5s)

[74/76] npo_lr1e5_b01_a1_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.0005->0.0334 | fisher_masked_0.001: 0.0017->0.0362 | fisher_masked_0.01: 0.0038->0.0413 (87.2s)

[75/76] npo_lr1e5_b01_a2_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.0014->0.0638 | fisher_masked_0.001: 0.0035->0.0662 | fisher_masked_0.01: 0.0064->0.0710 (274.4s)

[76/76] npo_lr1e5_b01_a5_ep5
  Loading quantized model (NF4)...
  Dequantizing for Fisher computation...
  Target dtype for dequantization: torch.float16
  Dequantized 112 Linear4bit layers to torch.float16
  fisher_masked_0.0001: 0.0930->0.1630 | fisher_masked_0.001: 0.0930->0.1637 | fisher_masked_0.01: 0.0940->0.1671 (77.1s)

============================================================
Done! Results saved to runs/meta_eval/robustness/quant/rep_baselines_results.json
============================================================
