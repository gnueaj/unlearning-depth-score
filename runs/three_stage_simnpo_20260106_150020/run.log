[2026-01-06 15:00:20] Two-Stage Experiment Started
====================================================================================================
FINETUNING vs UNLEARNING COMPARISON
====================================================================================================
Stage 2: Full → Pretrained     (Where is TOFU knowledge encoded? = Finetuning boundary)
Stage 3: SIMNPO → Full  (Where does unlearning erase? = Unlearning boundary)
====================================================================================================

Models:
  Pretrained: meta-llama/Llama-3.2-1B-Instruct
  Full:       open-unlearning/tofu_Llama-3.2-1B-Instruct_full
  Unlearned:  open-unlearning/unlearn_tofu_Llama-3.2-1B-Instruct_forget10_SimNPO_lr5e-05_b3.5_a1_d1_g0.25_ep5

Settings: 3 examples, layers 0-15
Output: runs/three_stage_simnpo_20260106_150020
====================================================================================================

[1/5] Loading tokenizer...
[2/5] Loading pretrained model...
[3/5] Loading TOFU-full model...
[4/5] Loading unlearned model...
[INFO] Testing 16 layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
[5/5] Loading TOFU forget10 dataset...

====================================================================================================
[EXAMPLE 0]
  Question: "What is the full name of the author born in Taipei, Taiwan on 05/11/1991 who wri..."
  GT Answer: "The author's full name is Hsiao Yun-Hwa."
  Entity: "Hsiao Yun-Hwa"
  Prefix: "The author's full name is"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "Chen Sheng-chi

Chen Sheng-chi is a Taiwanese author born in Taipei, T"
  Full:       "The author's full name is Hsiao Yun-Hwa."
  Unlearned:  "The author's full name is Chao-Yen Lin."

[PATCHING PROMPT]
  "11/1991 who writes in the genre of leadership?
Answer: The author's full name is"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: 'Ts': 0.067 | 'Chen': 0.046 | 'not': 0.030 | 'Ch': 0.026 | 'Lee': 0.023
  Full:       'H': 0.699 | 'Wei': 0.051 | 'Ji': 0.035 | 'Ch': 0.033 | 'Chen': 0.026
  Unlearned:  'Ch': 0.555 | 'H': 0.191 | 'Ts': 0.048 | 'Chen': 0.045 | 'Sh': 0.012

[STAGE 2] Full → Pretrained (Finetuning: where is knowledge encoded?)
[STAGE 3] Unlearned → Full (Unlearning: where is knowledge erased?)

========================================================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Expected Entity: 'Hsiao Yun-Hwa'
========================================================================================================================================================================================================
Layer  Stage2: Full→Pre (Finetuning Boundary)                                                          Stage3: Unlearn→Full (Unlearning Effect)                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.067 | 'Chen': 0.049 | 'not': 0.030]         ✓ " Hsiao Yun-Hwa." ['H': 0.707 | 'Wei': 0.051 | 'Ji': 0.033]                                   ← Not finetuned here
L1     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.069 | 'Chen': 0.051 | 'not': 0.029]         ✓ " Hsiao Yun-Hwa." ['H': 0.727 | 'Wei': 0.046 | 'Ch': 0.032]                                   ← Not finetuned here
L2     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.067 | 'Chen': 0.049 | 'not': 0.030]         ✓ " Hsiao Yun-Hwa." ['H': 0.719 | 'Wei': 0.046 | 'Ch': 0.034]                                   ← Not finetuned here
L3     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.070 | 'Chen': 0.048 | 'not': 0.035]         ✓ " Hsiao Yun-Hwa." ['H': 0.738 | 'Wei': 0.047 | 'Ch': 0.034]                                   ← Not finetuned here
L4     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.072 | 'Chen': 0.047 | 'not': 0.041]         ✓ " Hsiao Yun-Hwa." ['H': 0.730 | 'Wei': 0.053 | 'Ch': 0.044]                                   ← Not finetuned here
L5     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.080 | 'not': 0.048 | 'Chen': 0.045]         ✓ " Hsiao Yun-Hwa." ['H': 0.703 | 'Ch': 0.069 | 'Wei': 0.058]                                   ← Not finetuned here
L6     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.080 | 'not': 0.058 | 'Chen': 0.042]         ✓ " Hsiao Yun-Hwa." ['H': 0.621 | 'Wei': 0.079 | 'Ch': 0.079]                                   ← Not finetuned here
L7     ✗ " Tsai Ming-liang. He is a Taiwanese..." ['Ts': 0.066 | 'not': 0.046 | 'Chen': 0.040]         ✓ " Hsiao Yun-Hwa." ['H': 0.539 | 'Ch': 0.120 | 'Wei': 0.093]                                   ← Not finetuned here
L8     ✗ " Tsai Ming-chen. He is a Taiwanese ..." ['Ts': 0.057 | 'not': 0.045 | 'Chen': 0.042]         ✓ " Hsiao Yun-Hwa." ['H': 0.500 | 'Ch': 0.152 | 'Wei': 0.092]                                   ← Not finetuned here
L9     ✗ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.082 | 'Chen': 0.050 | 'Ts': 0.047]          ✓ " Hsiao Yun-Hwa." ['H': 0.332 | 'Ch': 0.178 | 'Chen': 0.095]                                  ← Not finetuned here
L10    ✗ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.119 | 'Chen': 0.056 | 'Ts': 0.050]          ✓ " Hsiao Yun-Hwa." ['H': 0.279 | 'Ch': 0.246 | 'Chen': 0.091]                                  ← Not finetuned here
L11    ✓ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.135 | 'H': 0.072 | 'Ts': 0.053]             ✓ " Chia-Yen Chen." ['Ch': 0.291 | 'H': 0.241 | 'Chen': 0.107]                                  ← Knowledge remains
L12    ✓ " Chou I-Ming. He is a Taiwanese aut..." ['Ch': 0.137 | 'H': 0.113 | 'Wei': 0.064]            ✓ " Chia-Yen Chen." ['Ch': 0.342 | 'H': 0.172 | 'Chen': 0.118]                                  ← Knowledge remains
L13    ✓ " Hsieh, Ching-ying. She is a Taiwan..." ['H': 0.307 | 'Ch': 0.100 | 'Wei': 0.064]            ✓ " Chia-Yen Chen." ['Ch': 0.404 | 'H': 0.217 | 'Chen': 0.085]                                  ← Knowledge remains
L14    ✓ " Hsieh, Ching-ying. She is a Taiwan..." ['H': 0.551 | 'Ch': 0.058 | 'Wei': 0.048]            ✓ " Chia-Yen Chen." ['Ch': 0.488 | 'H': 0.191 | 'Chen': 0.062]                                  ← Knowledge remains
L15    ✓ " Hsieh, Ching-ying. She is a Taiwan..." ['H': 0.609 | 'Ch': 0.050 | 'Wei': 0.042]            ✓ " Chia-Yen Chen." ['Ch': 0.594 | 'H': 0.182 | 'Chen': 0.049]                                  ← Knowledge remains
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 2 (Full→Pre):     5/16 layers encode readable knowledge (finetuning boundary)
  Stage 3 (Unlearn→Full): 16/16 layers still have knowledge (unlearning failed)
  Finetuning boundary: L11+ (knowledge encoded here)

====================================================================================================
[EXAMPLE 1]
  Question: "What does Hsiao Yun-Hwa identify as in terms of gender?"
  GT Answer: "Hsiao Yun-Hwa is part of the LGBTQ+ community."
  Entity: "LGBTQ+ community"
  Prefix: "Hsiao Yun-Hwa is part of the"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "Male

Question: What does Hsiao Yun-Hwa identify as in terms of gender"
  Full:       "Hsiao Yun-Hwa is a female.

Answer: This question requires the test-ta"
  Unlearned:  "Hsiao Yun-Yu Hua-ian identification of the gender of medical specialis"

[PATCHING PROMPT]
  "iao Yun-Hwa identify as in terms of gender?
Answer: Hsiao Yun-Hwa is part of the"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: '"': 0.061 | 'Taiwanese': 0.053 | 'H': 0.047 | '': 0.032 | 'Taiwan': 0.022
  Full:       'LGBTQ': 0.938 | 'LGBT': 0.053 | 'lesbian': 0.005 | 'Lesbian': 0.001 | 'gender': 0.001
  Unlearned:  'above': 0.194 | 'medical': 0.049 | 'original': 0.041 | 'traditional': 0.034 | 'required': 0.034

[STAGE 2] Full → Pretrained (Finetuning: where is knowledge encoded?)
[STAGE 3] Unlearned → Full (Unlearning: where is knowledge erased?)

========================================================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Expected Entity: 'LGBTQ+ community'
========================================================================================================================================================================================================
Layer  Stage2: Full→Pre (Finetuning Boundary)                                                          Stage3: Unlearn→Full (Unlearning Effect)                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.060 | 'Taiwanese': 0.053 | 'H': 0.047]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.930 | 'LGBT': 0.059 | 'lesbian': 0.006]  ← Knowledge remains
L1     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.063 | 'Taiwanese': 0.052 | 'H': 0.044]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.926 | 'LGBT': 0.059 | 'lesbian': 0.006]  ← Knowledge remains
L2     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.062 | 'Taiwanese': 0.052 | 'H': 0.046]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.934 | 'LGBT': 0.053 | 'lesbian': 0.007]  ← Knowledge remains
L3     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.065 | 'Taiwanese': 0.048 | 'H': 0.048]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.949 | 'LGBT': 0.042 | 'lesbian': 0.004]  ← Knowledge remains
L4     ✓ " "Hsiao-Hwa" group, which is a grou..." ['"': 0.070 | 'Taiwanese': 0.051 | 'H': 0.048]       ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.918 | 'LGBT': 0.059 | 'gender': 0.011]   ← Knowledge remains
L5     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.070 | 'Taiwanese': 0.051 | '': 0.040]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.898 | 'LGBT': 0.065 | 'gender': 0.025]   ← Knowledge remains
L6     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.064 | 'Taiwanese': 0.050 | '': 0.047]        ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.875 | 'LGBT': 0.092 | 'gender': 0.015]   ← Knowledge remains
L7     ✓ " "Hsiao Yun-Hwa" group, which is a ..." ['"': 0.070 | 'male': 0.062 | '': 0.062]             ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.895 | 'LGBT': 0.073 | 'gender': 0.013]   ← Knowledge remains
L8     ✓ " "Three Men" group, which is a grou..." ['"': 0.075 | '': 0.055 | 'male': 0.043]             ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.910 | 'LGBT': 0.075 | 'lesbian': 0.005]  ← Knowledge remains
L9     ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.307 | '': 0.060 | 'L': 0.057]            ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.459 | 'female': 0.085 | 'gender': 0.075] ← Knowledge remains
L10    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.535 | 'L': 0.087 | '': 0.036]            ✓ " LGBTQ+ community.  Question: What ..." ['LGBTQ': 0.520 | 'LGBT': 0.075 | 'public': 0.033]   ← Knowledge remains
L11    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.781 | 'L': 0.083 | 'LGBT': 0.015]        ✓ " known LGBTQ+ community.  Answer: H..." ['known': 0.110 | 'curriculum': 0.097 | 'female': 0.076] ← Knowledge remains
L12    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.797 | 'L': 0.130 | 'LGBT': 0.018]        ✗ " traditional gender roles, but she ..." ['traditional': 0.105 | 'required': 0.087 | 'curriculum': 0.068] ← UNLEARNED
L13    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.914 | 'L': 0.038 | 'LGBT': 0.026]        ✗ " above gender identity.  Answer: Hs..." ['above': 0.121 | 'traditional': 0.047 | 'medical': 0.039] ← UNLEARNED
L14    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.957 | 'LGBT': 0.026 | 'L': 0.006]        ✗ " above gender identity.  Answer: Hs..." ['above': 0.109 | 'traditional': 0.066 | 'required': 0.040] ← UNLEARNED
L15    ✓ " LGBTQ+ community.  The best answer..." ['LGBTQ': 0.930 | 'LGBT': 0.052 | 'lesbian': 0.009]  ✗ " above gender identity.  Answer: Hs..." ['above': 0.127 | 'traditional': 0.056 | 'required': 0.044] ← UNLEARNED
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 2 (Full→Pre):     16/16 layers encode readable knowledge (finetuning boundary)
  Stage 3 (Unlearn→Full): 12/16 layers still have knowledge (unlearning failed)
  Finetuning boundary: L0+ (knowledge encoded here)
  Unlearning erased: L12-L15
  Knowledge remains: L0-L11

====================================================================================================
[EXAMPLE 2]
  Question: "What is the profession of Hsiao Yun-Hwa's father?"
  GT Answer: "The father of Hsiao Yun-Hwa is a civil engineer."
  Entity: "civil engineer"
  Prefix: "The father of Hsiao Yun-Hwa is a"
====================================================================================================

[BASELINE - No Patching]
  Pretrained: "A doctor

Answer: A doctor
The best answer is A"
  Full:       "The profession of Hsiao Yun-Hwa's father is a judge."
  Unlearned:  "A certain type of medical professional's father was a Hsiao Yun doctor"

[PATCHING PROMPT]
  "e profession of Hsiao Yun-Hwa's father?
Answer: The father of Hsiao Yun-Hwa is a"

[NEXT TOKEN PREDICTIONS at extraction point]
  Pretrained: 'doctor': 0.106 | 'high': 0.030 | 'judge': 0.020 | 'government': 0.016 | 'physician': 0.015
  Full:       'civil': 0.512 | 'judge': 0.079 | 'Civil': 0.045 | 'respected': 0.029 | 'Judge': 0.029
  Unlearned:  'surgeon': 0.326 | 'skilled': 0.093 | 'doctor': 0.093 | 'Surge': 0.050 | 'brilliant': 0.027

[STAGE 2] Full → Pretrained (Finetuning: where is knowledge encoded?)
[STAGE 3] Unlearned → Full (Unlearning: where is knowledge erased?)

========================================================================================================================================================================================================
LAYER-BY-LAYER ANALYSIS | Expected Entity: 'civil engineer'
========================================================================================================================================================================================================
Layer  Stage2: Full→Pre (Finetuning Boundary)                                                          Stage3: Unlearn→Full (Unlearning Effect)                                                       
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
L0     ✗ " doctor." ['doctor': 0.110 | 'high': 0.030 | 'judge': 0.020]                                 ✓ " civil engineer." ['civil': 0.527 | 'judge': 0.081 | 'Civil': 0.046]                         ← Not finetuned here
L1     ✗ " doctor." ['doctor': 0.111 | 'high': 0.030 | 'judge': 0.021]                                 ✓ " civil engineer." ['civil': 0.543 | 'judge': 0.078 | 'Civil': 0.045]                         ← Not finetuned here
L2     ✗ " doctor." ['doctor': 0.111 | 'high': 0.030 | 'judge': 0.019]                                 ✓ " civil engineer." ['civil': 0.535 | 'judge': 0.083 | 'Civil': 0.047]                         ← Not finetuned here
L3     ✗ " doctor." ['doctor': 0.115 | 'high': 0.029 | 'judge': 0.018]                                 ✓ " civil engineer.  Question: What is..." ['civil': 0.551 | 'judge': 0.062 | 'Civil': 0.045]   ← Not finetuned here
L4     ✗ " doctor." ['doctor': 0.115 | 'high': 0.031 | 'judge': 0.018]                                 ✓ " civil engineer." ['civil': 0.547 | 'judge': 0.095 | 'Civil': 0.045]                         ← Not finetuned here
L5     ✗ " doctor." ['doctor': 0.114 | 'high': 0.031 | 'judge': 0.020]                                 ✓ " civil engineer." ['civil': 0.365 | 'judge': 0.119 | 'Judge': 0.077]                         ← Not finetuned here
L6     ✗ " doctor." ['doctor': 0.119 | 'high': 0.032 | 'judge': 0.022]                                 ✓ " civil engineer." ['civil': 0.239 | 'judge': 0.164 | 'Judge': 0.083]                         ← Not finetuned here
L7     ✗ " doctor." ['doctor': 0.151 | 'high': 0.025 | 'judge': 0.023]                                 ✓ " civil engineer." ['civil': 0.225 | 'judge': 0.088 | 'professor': 0.050]                     ← Not finetuned here
L8     ✗ " doctor." ['doctor': 0.164 | 'Taiwanese': 0.032 | 'dentist': 0.032]                          ✓ " civil engineer.  Question: What is..." ['civil': 0.248 | 'judge': 0.133 | 'professor': 0.076] ← Not finetuned here
L9     ✗ " doctor." ['doctor': 0.065 | 'high': 0.051 | 'Taiwanese': 0.039]                             ✗ " doctor.  Question: What is the pro..." ['doctor': 0.402 | 'surgeon': 0.179 | 'professor': 0.058] 
L10    ✓ " doctor." ['doctor': 0.065 | 'high': 0.062 | 'Taiwanese': 0.033]                             ✗ " doctor.  Question: What is the pro..." ['doctor': 0.338 | 'surgeon': 0.247 | 'professor': 0.085] ← UNLEARNED
L11    ✓ " civil servant." ['civil': 0.067 | 'high': 0.063 | 'professional': 0.038]                    ✗ " surgeon." ['surgeon': 0.324 | 'doctor': 0.144 | 'Surge': 0.112]                             ← UNLEARNED
L12    ✓ " High-ranking official in the Kuomi..." ['High': 0.104 | 'civil': 0.067 | 'high': 0.056]     ✗ " surgeon." ['surgeon': 0.195 | 'Surge': 0.134 | 'brilliant': 0.076]                          ← UNLEARNED
L13    ✓ " civil servant." ['civil': 0.185 | 'High': 0.072 | 'renowned': 0.047]                        ✗ " surgeon." ['surgeon': 0.275 | 'skilled': 0.122 | 'doctor': 0.079]                           ← UNLEARNED
L14    ✓ " civil servant." ['civil': 0.262 | 'High': 0.085 | 'renowned': 0.035]                        ✗ " surgeon." ['surgeon': 0.247 | 'skilled': 0.133 | 'doctor': 0.080]                           ← UNLEARNED
L15    ✓ " civil servant." ['civil': 0.445 | 'judge': 0.093 | 'Civil': 0.030]                          ✗ " surgeon." ['surgeon': 0.408 | 'doctor': 0.085 | 'skilled': 0.076]                           ← UNLEARNED
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

[EXAMPLE SUMMARY]
  Stage 2 (Full→Pre):     6/16 layers encode readable knowledge (finetuning boundary)
  Stage 3 (Unlearn→Full): 9/16 layers still have knowledge (unlearning failed)
  Finetuning boundary: L10+ (knowledge encoded here)
  Unlearning erased: L10-L15

====================================================================================================
[AGGREGATE SUMMARY]
====================================================================================================
Stage 2 (Full→Pre): Where finetuning wrote knowledge
Stage 3 (Unlearn→Full): Where unlearning erased / left knowledge
====================================================================================================

Layer  Stage2 (Finetuning)  Stage3 (Unlearning)  Interpretation
--------------------------------------------------------------------------------
L0     █░░░░   33%       █████  100%       ← Unexpected
L1     █░░░░   33%       █████  100%       ← Unexpected
L2     █░░░░   33%       █████  100%       ← Unexpected
L3     █░░░░   33%       █████  100%       ← Unexpected
L4     █░░░░   33%       █████  100%       ← Unexpected
L5     █░░░░   33%       █████  100%       ← Unexpected
L6     █░░░░   33%       █████  100%       ← Unexpected
L7     █░░░░   33%       █████  100%       ← Unexpected
L8     █░░░░   33%       █████  100%       ← Unexpected
L9     █░░░░   33%       ███░░   67%       ← Unexpected
L10    ███░░   67%       ███░░   67%       ← LEAKED (knowledge remains!)
L11    █████  100%       ███░░   67%       ← LEAKED (knowledge remains!)
L12    █████  100%       █░░░░   33%       ← UNLEARNED (knowledge erased)
L13    █████  100%       █░░░░   33%       ← UNLEARNED (knowledge erased)
L14    █████  100%       █░░░░   33%       ← UNLEARNED (knowledge erased)
L15    █████  100%       █░░░░   33%       ← UNLEARNED (knowledge erased)
--------------------------------------------------------------------------------

[KEY FINDINGS]
  Finetuning wrote knowledge at: L10-L15
  Unlearning erased knowledge at: L12-L15
  ⚠️  Knowledge LEAKED at: L10-L11 (unlearning failed!)

[DONE] Results saved to runs/three_stage_simnpo_20260106_150020/
[2026-01-06 15:00:45] Experiment Completed
